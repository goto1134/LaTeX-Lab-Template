	\section{Цель работы}
		\begin{enumerate}
			\item Вычислить латентность и пропускную способность при передаче данных с помощью функций $ MPI\_Send/MPI\_Recv $ . Учесть, что обмен при выполнении программы может на самом деле происходить как между процессорами одного хоста, так и между хостами.
			\item Сравнить производительность функций $ MPI\_Bcast $ и $ MPI\_Send/MPI\_Recv $ при разном количестве задействованных узлов.
			\item Сравнить производительность функций $ MPI\_Reduce $ и $ MPI\_Send / MPI\_Recv $ при разном количестве задействованных узлов.
		\end{enumerate}		
	\section{Порядок выполенения работы}
		\subsection{Вычисление латентности}
			Латентность измеряется как время, необходимое на передачу сигнала, или сообщения нулевой длины. При этом, для снижения влияния погрешности и низкого разрешения системного таймера, важно повторить операцию посылки сигнала и получения ответа большое число $ N $ раз. Таким образом, если время на $ N $ итераций пересылки сообщений нулевой длины между двумя участниками составило $ T $ сек., то латентность измеряется как $s=T/(2N)$.
			
			В результате для $ N = 10000000 $ латентность $ s_1 = 0.001 $ мс для узлов, находящихся на одном хосте, и $ s_2 = 0.005 $ мс для узлов на разных хостах.
			
		\subsection{Вычисление пропускной способности}
			Для измерения пропускной способности процесс с номером 0 посылает процессу с номером 1 сообщение длины $ L $ байт. Процесс 1, приняв сообщение от процесса 0, посылает ему ответное сообщение той же длины. Используются блокирующие вызовы MPI. Эти действия повторяются $ N $ раз с целью минимизировать погрешность за счет усреднения. Процесс 0 измеряет время T, затраченное на все эти обмены. Пропускная способность $ R $ определяется по формуле $ R=2NL/T $.
			
			При $ N = 1000000 $ и $L = 1 $ Мб пропускная способность $P_1 = 960$ MБ/сек. для узлов, находящихся на одном хосте, и $P_2 = 660$ MБ/сек. для узлов на разных хостах.
			
		\subsection{Сравнение производительности $ MPI\_Bcast $ и $ MPI\_Send/MPI\_Recv $}
			\label{sec:bcast}
			Для сравнения производительности производится $ N $ итераций отправки сообщения нулевой длины с помощью команд $ MPI\_Send/MPI\_Recv $ поочерёдно на все узлы кластера и столько же операций с помощью команды $ MPI\_Bcast $. Производительность измеряется, как $ P = N / T $, где $ T $ - время выполнения $ N $ итераций оповещения.
			
			
			Как видно из таблицы \ref{talbe:bcast}, функция $ MPI\_Bcast $ имеет б\`{о}льшую производительность, почти постоянную, когда при использовании функций $ MPI\_Send/MPI\_Recv $ время вещания на два порядка больше, а производительность быстро убывает с ростом числа узлов.
			\begin{table}
				\caption{Сравнение производительности $ MPI\_Bcast $ и $ MPI\_Send/MPI\_Recv $ для $ N = 1000000$}
				\label{talbe:bcast}
				\begin{tabular}{|c|c|c|c|c|}
					\hline 
					& \multicolumn{2}{c|}{Send/Receive}  & \multicolumn{2}{c|}{Broadcast} \\ 
					\hline 
					\multicolumn{1}{|m{2.5cm}|}{Количество узлов} & T, сек. & P, оп./сек. & T, сек. & P, оп./сек. \\ 
					\hline 
					2 & $11 $ & $ 8.87 \cdot 10^4 $ & $ 1.55 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline 
					3 & $14 $ & $ 7.03 \cdot 10^4 $ & $ 1.55 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline 
					4 & $15 $ & $ 6.64 \cdot 10^4 $ & $ 1.55 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline 
					5 & $16 $ & $ 6.19 \cdot 10^4 $ & $ 1.56 \cdot 10^{-2} $ & $6.42 \cdot 10^6$ \\ 
					\hline 
					6 & $18 $ & $ 5.50 \cdot 10^4 $ & $ 1.56 \cdot 10^{-2} $ & $6.43 \cdot 10^6$ \\ 
					\hline 
					7 & $22 $ & $ 5.50 \cdot 10^4 $ & $ 1.56 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline 
					8 & $22.5 $ & $ 4.45 \cdot 10^4 $ & $ 1.56 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline 
					9 & $39 $ & $ 2.56 \cdot 10^4 $ & $ 1.56 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline 
					10 & $27 $ & $ 3.68 \cdot 10^4 $ & $ 1.56 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline
					11 & $28 $ & $ 3.61 \cdot 10^4 $ & $ 1.56 \cdot 10^{-2} $ & $6.41 \cdot 10^6$ \\  
					\hline 
					12 & $32 $ & $ 3.14 \cdot 10^4 $ & $ 1.57 \cdot 10^{-2} $ & $6.36 \cdot 10^6$ \\  
					\hline 
					13 & $38 $ & $ 2.64 \cdot 10^4 $ & $ 1.55 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline 
					14 & $43 $ & $ 2.32 \cdot 10^4 $ & $ 1.55 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline 
					15 & $48 $ & $ 2.05 \cdot 10^4 $ & $ 1.55 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline 
					16 & $48 $ & $ 2.06 \cdot 10^4 $ & $ 1.55 \cdot 10^{-2} $ & $6.44 \cdot 10^6$ \\ 
					\hline 
				\end{tabular} 
			\end{table}
		
		\FloatBarrier
			
		\subsection{Сравнение производительности  $ MPI\_Reduce $ и $ MPI\_Send / MPI\_Recv $}
			
			Для сравнения  производительности функций $ MPI\_Reduce $ и $ MPI\_Send / MPI\_Recv $
			рассмотрим задачу суммирования рангов всех процессоров, участвующих во взаимодействии. Как и в \ref{sec:bcast}, производительность измеряется, как $ P = N / T $, где $ T $ - время выполнения $ N $ итераций оповещения.
			
			Как видно из таблицы \ref{talbe:reduce}, производительность функции $ MPI\_Reduce $ почти не отличается от производительности $ MPI\_Send/MPI\_Recv $ для количества узлов $ < 8 $, а для большего количества $ MPI\_Reduce $  даёт существенный прирост производительности.
			\begin{table}
				\caption{Сравнение производительности $ MPI\_Reduce $ и $ MPI\_Send / MPI\_Recv $ для $ N = 1000000$}
				\label{talbe:reduce}
				\begin{tabular}{|c|c|c|c|c|}
					\hline 
					& \multicolumn{2}{c|}{Send/Receive}  & \multicolumn{2}{c|}{Reduce} \\ 
					\hline 
					\multicolumn{1}{|m{2.5cm}|}{Количество узлов} & T, сек. & P, оп./сек. & T, сек. & P, оп./сек. \\ 
					\hline 
					2 & $11 $ & $ 8.87 \cdot 10^4 $ & $ 11 $ & $ 8.88 \cdot 10^4 $ \\ 
					\hline 
					3 & $22 $ & $ 4.54 \cdot 10^4 $ & $ 22 $ & $ 4.55 \cdot 10^4 $ \\ 
					\hline 
					4 & $33 $ & $ 3.05 \cdot 10^4 $ & $ 33 $ & $ 3.06 \cdot 10^4 $ \\ 
					\hline 
					5 & $43 $ & $ 2.32 \cdot 10^4 $ & $ 43 $ & $ 2.31 \cdot 10^4 $ \\ 
					\hline 
					6 & $ 54 $ & $ 1.86 \cdot 10^4 $ & $ 54 $ & $ 1.85 \cdot 10^4 $ \\ 
					\hline 
					7 & $ 64 $ & $ 1.56 \cdot 10^4 $ & $ 64 $ & $ 1.56 \cdot 10^4 $ \\ 
					\hline 
					8 & $ 75 $ & $ 1.34 \cdot 10^4 $ & $ 70 $ & $ 1.42 \cdot 10^4 $ \\ 
					\hline 
					9 & $ 85 $ & $ 1.17 \cdot 10^4 $ & $ 59 $ & $ 1.7 \cdot 10^4 $ \\ 
					\hline 
					10 & $ 96 $ & $ 1.04 \cdot 10^4 $ & $ 64 $ & $ 1.56 \cdot 10^4 $ \\ 
					\hline
					11 & $ 107 $ & $ 0.93 \cdot 10^4 $ & $ 81 $ & $ 1.24 \cdot 10^4 $ \\ 
					\hline 
					12 & $ 117 $ & $ 0.85 \cdot 10^4 $ & $ 116 $ & $ 0.86 \cdot 10^4 $ \\ 
					\hline 
					13 & $ 128 $ & $ 0.78 \cdot 10^4 $ & $ 71 $ & $ 1.4 \cdot 10^4 $ \\ 
					\hline 
					14 & $ 139 $ & $ 0.719 \cdot 10^4 $ & $ 138 $ & $ 0.723 \cdot 10^4 $ \\ 
					\hline 
					15 & $ 150 $ & $ 0.67 \cdot 10^4 $ & $ 80 $ & $ 1.25 \cdot 10^4 $ \\ 
					\hline 
					16 & $ 165 $ & $ 0.63 \cdot 10^4 $ & $ 77 $ & $ 1.3 \cdot 10^4 $ \\ 
					\hline 
					64 & $ 762 $ & $ 0.13 \cdot 10^4 $ & $ 166 $ & $ 0.6 \cdot 10^4 $ \\ 
					\hline 
				\end{tabular} 
			\end{table}
			
	\section{Вывод}
		В ходе работы
		\begin{enumerate}
			\item Изучены основы MPI.
			\item Вычислена латентность и пропускную способность при передаче данных с помощью функций $ MPI\_Send/MPI\_Recv $.
			\item Проведено сравнение производительность функций $ MPI\_Bcast $ и $ MPI\_Send/MPI\_Recv $ при разном количестве задействованных узлов.
			\item Проведено сравнение производительность функций $ MPI\_Reduce $ и $ MPI\_Send / MPI\_Recv $ при разном количестве задействованных узлов.
		\end{enumerate}
	
		Исходный код программ доступен по ссылке \href{https://github.com/goto1134/MPI-labs}{https://github.com/goto1134/MPI-labs}.